# Sample Efficient Reinforcement Learning (SEARL) - baselines
Stable-Baselines implementations have become the go-to for quick and baseline testing of basic algorithms in Model Free Reinforcement Learning (MFRL).

Through Sample Efficient Reinforcement Learning (SEARL), we intend to construct a similar alternative for algorithms in Model-Based Reinforcement Learning (MBRL) and Meta Reinforcment Learning (Meta-RL) fields.

# Algorithms

## Meta-RL
MQL [Under Construction]

PEARL [Under Construction]

MB-MPO [Under Construction]

# Dependencies
gym

pytorch

# Environments
1. CartPole
2. MountainCar
3. Ant
4. Walker
5. Humanoid
6. Half-Cheetah
7. Swimmer
8. [ProcGen](https://github.com/openai/procgen)

# Other Repos
## Note that this repo contains a lot of code copied from these implementations, so most of the credit goes to these devs.
1. [Model Based Reinforcement Learning Benchmarking Library (MBBL)](https://github.com/WilsonWangTHU/mbbl)
2. [PEARL Implementation](https://github.com/katerakelly/oyster)
